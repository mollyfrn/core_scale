#Figures and tables 
#wd1: setwd("C:/git/core_scale")
library(viridis)
library(tidyverse)
library(raster)
library(maps)
library(sp)
library(rgdal)
library(maptools)
library(zoo)
library(gridExtra)



####Dummy data and real data pre-plotting tidy for figures 1 & 4####
fifty_allyears = read.csv("intermed/fifty_allyears.csv", header = TRUE) #using updated version, 50 stop data, 07/12
bbs_allscales = read.csv("intermed/bbs_allscales.csv", header = TRUE)

fifty_bestAous = fifty_allyears %>% 
  filter(aou > 2880 & !(aou >= 3650 & aou <= 3810) & !(aou >= 3900 & aou <= 3910) & 
           !(aou >= 4160 & aou <= 4210) & aou != 7010) #leaving out owls, waterbirds as less reliable data

#use occ_counts function for calculating occupancy at any scale to get raw occs for distribution plots
occ_counts = function(countData, countColumns, scale) {
  bbssub = countData[, c("stateroute", "year", "aou", countColumns)] #these are our grouping vars
  bbssub$groupCount = rowSums(bbssub[, countColumns]) 
  bbsu = unique(bbssub[bbssub[, "groupCount"]!= 0, c("stateroute", "year", "aou")]) 
  
  abun.summ = bbssub %>% #abundance
    group_by(stateroute, year) %>%  
    summarize(totalN = sum(groupCount))  #we want to go further and summarize across focal + secondary rtes tho
  
  occ.summ = bbsu %>% #occupancy
    count(stateroute, aou) %>%
    mutate(occ = n/15, scale = scale) %>% #, #may want to get rid of, this is at the column-counting scale
    #scale = scale) %>%
    left_join(abun.summ, by = 'stateroute')
  return(occ.summ)
}


# Generic calculation of occupancy for a specified scale
#fix to run all at once, so no sep run for above-scale, USE occ-counts for both 
b_scales = c(5, 10, 25, 50)
output = c()
for (s in b_scales) {
  numGroups = floor(50/s)
  for (g in 1:numGroups) {
    groupedCols = paste("stop", ((g-1)*s + 1):(g*s), sep = "")
    temp = occ_counts(fifty_bestAous, groupedCols, s) 
    output = rbind(output, temp) 
  } 
}

min_dist = output
min_dist = min_dist[, -3] #removes vestigial "n" count column 

#need to avg occs between unique stateroute-aou pairs since 5 for every 1 
min_dist3 = min_dist %>% 
  group_by(aou, stateroute, scale) %>% 
  summarise(occ = mean(occ)) %>% dplyr::select(everything()) 
min_out = as.data.frame(min_dist3)

write.csv(min_out, "intermed/min_out.csv", row.names = FALSE)

#check 
min_out2 = min_out %>% 
  filter(scale == "50")

fig1a = ggplot(min_out2, aes(occ))+
  geom_density(bw = "bcv", kernel = "gaussian", n = 2000, na.rm = TRUE)+
  labs(x = "Proportion of time present at site", y = "Probability Density", title = "Single Route Scale")+ 
  theme_classic() #coord_cartesian(xlim = c(0, 1), ylim = c(0, 2.5))+
fig1a #looks good!

#######################

#distribution at the 2-66 rte group scales#
dist.df = read.csv("intermed/dist_df.csv", header = TRUE)
bbs_above_guide = read.csv("intermed/bbs_above_guide.csv", header = TRUE) #generated by core_scale_analyses
#groupcounts for each aou for each year at scale of ONE stateroute 

uniqrtes = unique(bbs_above_guide$stateroute) #all routes present are unique, still 953 which is great
scales = c(2, 4, 8, 16, 32, 66) 
max_out = c()

for (nu in scales){
  #test example route 2010 and nu at 57 routes -> large scale, should have high occ 
  for (r in uniqrtes) { #for each focal route
    tmp_rte_group = dist.df %>% #changes with size of nu but caps at 66
      filter(rte1 == r) %>% 
      top_n(66, desc(dist)) %>% #fixed ordering by including arrange parm, 
      #remove/skip top row 
      arrange(dist) %>%
      slice(1:nu) %>% 
      dplyr::select(everything()) %>% data.frame()
    
    
    focal_clustr = bbs_above_guide %>% 
      filter(stateroute %in% tmp_rte_group$rte2) 
    
    occ.summ = focal_clustr %>% 
      dplyr::select(year, aou) %>% #duplicates remnant of distinct secondary routes - finally ID'd bug
      distinct() %>% #removing duplicates 09/20
      count(aou) %>% #how many times does that aou show up in that clustr that year 
      dplyr::mutate(occ = n/15, stateroute = r, scale = nu) 
    
    max_out = rbind(max_out, occ.summ)
    
  }
}

max_out = max_out[, -2] #rm vistigal "n" column 
max_out = as.data.frame(max_out)

fig1c = ggplot(max_out, aes(occ))+
  geom_density(bw = "bcv", kernel = "gaussian", n = 2000, na.rm = TRUE)+
  labs(x = "Proportion of time present at site", y = "Probability Density", title = "Maximum Scale")+theme_classic()
#so it was the limits giving me crap in the original 
fig1c

write.csv(max_out, "intermed/max_out.csv", row.names = FALSE)
##################

#read in occ data for merge for all_fig output creation 
min_out = read.csv("intermed/min_out.csv", header = TRUE)
max_out = read.csv("intermed/max_out.csv", header = TRUE)
dist.df = read.csv("intermed/dist_df.csv", header = TRUE)
#groupcounts for each aou for each year at scale of ONE stateroute 

#filter out to only routes that are up to 1000km radius away from each other before analyses 
far = dist.df %>% arrange(rte1, dist) %>% group_by(rte1) %>% slice(66)
hist(far$dist)
far2 = far %>% filter(dist < 1000)

min_out2 = min_out %>% filter(stateroute %in% far2$rte1)
max_out2 = max_out %>% filter(stateroute %in% far2$rte1)
#updated 05/07

#organize by scales; label and differentiate scales so that below-rtes are appropriately smaller
min_out = min_out2 %>% 
  dplyr::select(stateroute, aou, occ, scale) %>% 
  dplyr::mutate(area = scale*(pi*(0.4^2))) %>% #scale corresponds to the number of stops
  dplyr::select(stateroute, aou, occ, area)

max_out = max_out2 %>% 
  dplyr::select(stateroute, aou, occ, scale) %>% 
  dplyr::mutate(area = scale*50*(pi*(0.4^2))) %>% #scale corresponds to the number of agg routes; 50 stops per rte
  dplyr::select(stateroute, aou, occ, area)


all_fig = rbind(max_out, min_out)
length(unique(all_fig$stateroute)) #983, as it should be 
write.csv(all_fig, "intermed/all_figoutput.csv", row.names = FALSE)
write.csv(max_out, "intermed/max_out.csv", row.names = FALSE)
write.csv(min_out, "intermed/min_out.csv", row.names = FALSE)
#stored in intermed folder 

####################################################################################################
####Figure 1: Bimodal dist images; number of spp on y vs # years present####  
#A: original bimodal dist 
#B: distribution at smallest scales 
#C: distribution at max scale 

#refer to coylefig1a.R script in core-transient scripts folder for guidance 
#need to recreate spp_matrix with current data 
#aou codes columns following first stateroute column 
#individual occ values for spp at each stateroute across the 15 year window 


#base fig1 
min_out = read.csv("intermed/min_out.csv", header = TRUE)

#filter to scale == 50, check
single_rte = min_out %>% 
  filter(area > 25) #983 unique routes

minplot = ggplot(single_rte, aes(occ))+
  stat_density(geom = "path", position = "identity", bw = "bcv", kernel = "gaussian", n = 4000, na.rm = TRUE, size = 1.3)+
  labs(x = "Proportion of time present at site", y = "Probability Density")+theme_classic()+
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 2.5))+
  theme(axis.title = element_text(size = 18))+theme(legend.position = c(0.50, 0.50))
minplot

local = rollmean(rexp(n = 76179, rate = 12), k = 5, fill = "extend")
big = rollmean(rexp(n = 76179, rate= 139/144), k = 3000, fill = "extend")

big2 = local+0.9
big3 = sort(big2)
big4 = c(big3[1:200] - 0.1346, big3[201:400] - 0.0970, big3[401:602] - 0.07958,
         big3[603:1600] - 0.06898, big3[1601:2000] - 0.05340,  big3[2001:2400] - 0.0458, 
         big3[2401:2800] - 0.01287, big3[2801:3200] - 0.00167, big3[3201:length(big3)])
big5 = rollmean(big4, k = 500, fill = "extend")
big6 = rollmean(big5, k = 5, fill = "extend")
preds = cbind(single_rte, local)
pred_dist = cbind(preds, big6)


all_predplot = ggplot(pred_dist, aes(occ))+
  geom_rect(aes(xmin = 0, ymin = 0, xmax = 0.33, ymax = 6), fill = "grey", alpha = 0.02)+
  geom_rect(aes(xmin = .67, ymin = 0, xmax = 1, ymax = 6), fill = "grey", alpha = 0.02)+
  stat_density(aes(color = "Observed"), geom = "path", position = "identity", bw = "bcv", kernel = "gaussian", n = 4000, na.rm = TRUE, size = 1.5)+
  stat_density(aes(local, color = "Small scale"), geom = "path", position = "identity", bw = "bcv", kernel = "gaussian", n = 4000, na.rm = TRUE, size = 1.5, linetype = "dashed")+
  stat_density(aes(big6, color = "Large scale"), geom = "path", position = "identity", bw = "bcv", kernel = "gaussian", n = 24, na.rm = TRUE, size = 1.5, linetype = "dashed")+
  labs(x = "Proportion of time present at site", y = "Probability Density")+theme_classic()+
  theme(axis.title = element_text(size = 18), axis.text = element_text(size = 16))+ 
  coord_cartesian(xlim = c(0.11, .95), ylim = c(0, 5.5))
all_predplot + scale_color_manual(values = c("Observed" = "black", "Small scale" = "#287D8EFF", "Large scale" = "#FDE725FF"))+
  theme(legend.position = c(0.45, 0.45), legend.text = element_text(size = 16), legend.title = element_blank())
ggsave(file = "output/Figure1.tiff", plot = last_plot())
####Fig 2 schematics generated in powerpoint####

####Fig 3####
##Make background grey, illustrate 66 points region in black, with red star centerpt 
NorthAm = readOGR(dsn = "map_outline", layer = "continent")
NorthAm2 = spTransform(NorthAm, CRS("+proj=laea +lat_0=45.235 +lon_0=-106.675 +units=km"))
bbs_latlon = read.csv("intermed/good_rtes2.csv", header = TRUE)
dist.df = read.csv("intermed/dist_df.csv", header = TRUE)

dist.df_sub = dist.df %>% 
  filter(rte1 == "2001")%>% 
  top_n(66, desc(dist)) %>% 
  arrange(dist) 

dist.df_sub2 = dist.df %>% 
  filter(rte1 == "89152") %>%
  top_n(66, desc(dist)) %>% 
  arrange(dist) 

#exclude routes that have missing above OR below scale data, such that sites are only calculated for routes that cover all 83 scales
bbs_allscales = read.csv("intermed/bbs_allscales.csv", header = TRUE)
bbs_latlon = filter(bbs_latlon, stateroute %in% bbs_allscales$focalrte)
bbs_latlon$stateroute = as.character(bbs_latlon$stateroute)
bbs_secnd = filter(bbs_latlon, stateroute %in% dist.df_sub$rte2) #things get out of order! 
bbs_thrd = filter(bbs_latlon, stateroute %in% dist.df_sub2$rte2)

sites1 = data.frame(longitude = bbs_secnd$longitude, latitude = bbs_secnd$latitude) 
sites2 = data.frame(longitude = bbs_thrd$longitude, latitude = bbs_thrd$latitude) 
star1 = bbs_secnd %>% filter(stateroute == "2001")
star2 = bbs_thrd %>% filter(stateroute == "89152")


tiff(file = "output/E_W_comparison.tiff")
plot(NorthAm, xlim = c(-160, -60), ylim = c(25, 65))
points(bbs_latlon$longitude, bbs_latlon$latitude, col= "grey", pch=16)
points(sites1$longitude, sites1$latitude, col = "#FDE725FF", pch = 16)
points(sites2$longitude, sites2$latitude, col = viridis(1, begin = 0.5, end = 1, option = "D"), pch = 16)
points(star1$longitude, star1$latitude, col = "black", pch = 17, cex = 2)
points(star2$longitude, star2$latitude, col = "black", pch = 17, cex = 2)
dev.off()

####################################################################################################
####Results section figs####

####Figure 4: Plotting how CT distributions change across scale, using area####
all_fig = read.csv("intermed/all_figoutput.csv", header = TRUE)
#all_fig$area = as.factor(all_fig$area)
all_fig$area_f = factor(round(all_fig$area),
                        levels = c(3, 5, 13, 25, 50, 101, 201, 402, 804, 1659), 
                        labels = c("2.5, 5 point count stops", "5", "13", "25, 1 BBS route", "50", "101", "201", "402", "804", "1659, 66 aggregate BBS routes")) 
all_fig = all_fig %>% 
  #first I have to take levels for area_f and lump everything together that isn't 50/25, 1 BBS route
  mutate(area_spec = as.numeric(area_f))

all_fig$area_spec[all_fig$area_spec != 4] <- 0

all_fig$area_spec = factor(all_fig$area_spec)


all_figplot = ggplot(all_fig, aes(occ, group = area_f, color = area_f, linetype = area_spec))+
  stat_density(geom = "path", position = "identity", bw = "bcv", kernel = "gaussian", n = 4000, na.rm = TRUE, size = 1.3)+
  #stat_density(singlerte, aes(occ), geom = "path", position = "identity", bw = "bcv", kernel = "gaussian", n = 4000, na.rm = TRUE, size = 1.6)+
  labs(x = "Proportion of time present at site", y = "Probability Density")+theme_classic()+
  scale_color_viridis(discrete = TRUE, name = expression("Spatial Scale in km"^{2}))+
  theme(axis.title = element_text(size = 18), axis.text = element_text(size = 16))+
  theme(legend.text = element_text(size = 16), legend.title = element_text(size = 16))+
  theme(legend.position = c(0.50, 0.50))+guides(linetype = FALSE)
all_figplot
ggsave(file = "output/Figure4.tiff", plot = all_figplot)


####Figure 5A & B, with rollmeans moving window avgs####
####Plotting NULL all routes with 3 highlighted "types####
bbs_allscales = read.csv("intermed/bbs_allscales.csv", header = TRUE)
bbs_allscales = bbs_allscales %>% 
  dplyr::filter(logN != "NA")

core_coefs = read.csv("intermed/core_coefs.csv", header = TRUE) #AUC etc.

coefs_ranked = core_coefs %>% 
  arrange(PCA.curvature) #middle teal line should be least curvy

coefs_avgs = core_coefs %>% 
  summarise_all(funs(mean, sd)) %>% 
  mutate(focalrte = "99999") 


#compare rtes in homogeneous vs heterogeneous regions, illustrate in color to prove point 
env_all = read.csv("intermed/env_all.csv", header = TRUE) #AUC etc.
ndvi_ranked = env_all %>% 
  group_by(stateroute) %>% 
  summarize(ndvi_m = mean(ndvi.var)) %>%
  arrange(desc(ndvi_m)) 
#lowest var in NDVI: rtes 72151, 72049, 72052, #mostly 72's and 2,000's 
#highest var in NDVI: rtes 14059, 14140, 69253, 69021, 85011

elev_ranked = env_all %>% 
  group_by(stateroute) %>% 
  summarize(elev_m = mean(elev.var)) %>%
  arrange(desc(elev_m))
#lowest var in elev: rtes 34027 (best, closest to normal avgs), mostly 34's, 35010, 
#highest var in elev: rtes 17221, 6012, 17044, 6071, 85169, 14059 mostly 14's, 17's, and 6,000's

central_alt = bbs_allscales %>%  
  dplyr::select(logA, pctCore) %>% 
  transmute(pctCore_m = rollapply(pctCore, width = 1, FUN = mean, na.rm = TRUE, fill = NULL),
            logA = logA) %>% 
  mutate(logA = round(logA, digits = 2)) %>%
  group_by(logA) %>%
  summarise(pctCore = mean(pctCore_m)) %>% 
  mutate(focalrte = "99999", logA = logA) %>% 
  dplyr::select(focalrte, logA, pctCore)

bbs_allsub = bbs_allscales %>% 
  filter(focalrte == 34054 | focalrte == 85169) %>%
  dplyr::select(focalrte, logA, pctCore)

bbs_allsub2 = rbind(bbs_allsub, central_alt)

bbs_allsub2$focalrte = factor(bbs_allsub2$focalrte,
                              levels=c( "99999","34054", "85169"),
                              labels=c("Mean",
                                       "Low Heterogeneity",
                                       "High Heterogeneity"))
#use this to assign diff colors for each factor level per what color scheme is ideal?
#72 is PA, 14 is Cali, 34 is Illinois, 17 is Colorado 

pred_plot = ggplot(bbs_allscales, aes(x = logA, y = pctCore))+geom_line(aes(group = focalrte), color = "grey")+
  theme_classic()+geom_abline(aes(intercept = 0.5, slope = 0), linetype = "dashed")+
  geom_line(data = bbs_allsub2, aes(x = logA, y = pctCore, group = as.factor(focalrte), color = as.factor(focalrte)), size = 2)+ #geom_smooth(model = lm, color = 'red')+
  labs(x = "Log Area", y = "", title = "A")+
  scale_color_viridis(discrete = TRUE, name = "", option = "B", begin = 0.05, end = .75)+
  theme(plot.title = element_text(size = 20), axis.title = element_text(size = 18), axis.text = element_text(size = 16), legend.text = element_text(size = 14), legend.title = element_text(size = 14))+
  theme(legend.position = c(0.74, 0.18)) 
pred_plot #yellow = high variation in habhet, purple = low variation, low habhet 

central2_alt = bbs_allscales %>%  
  dplyr::select(logN, pctCore) %>% 
  transmute(pctCore_m = rollapply(pctCore, width = 1, FUN = mean, na.rm = TRUE, fill = NULL),
            logN = logN) %>% 
  mutate(logN = round(logN, digits = 1)) %>%
  group_by(logN) %>%
  summarise(pctCore = mean(pctCore_m)) %>% 
  mutate(focalrte = "99999", logN = logN) %>% 
  dplyr::select(focalrte, logN, pctCore)

#calculate confidence intervals for central_alt and central2_alt vals


bbs_allsub = bbs_allscales %>% 
  filter(focalrte == 34054 | focalrte == 85169) %>%
  dplyr::select(focalrte, logN, pctCore)

bbs_allsub3 = rbind(bbs_allsub, central2_alt)

bbs_allsub3$focalrte = factor(bbs_allsub3$focalrte,
                              levels=c( "99999","34054", "85169"),
                              labels=c("Mean",
                                       "Low Heterogeneity",
                                       "High Heterogeneity"))
#use this to assign diff colors for each factor level per what color scheme is ideal?
#72 is PA, 14 is Cali, 34 is Illinois, 17 is Colorado 

pred_abuns = ggplot(bbs_allscales, aes(x = logN, y = pctCore))+geom_line(aes(group = focalrte), color = "grey")+
  theme_classic()+geom_abline(aes(intercept = 0.5, slope = 0), linetype = "dashed")+
  geom_line(data = bbs_allsub3, aes(x = logN, y = pctCore, group = as.factor(focalrte), color = as.factor(focalrte)), size = 2)+ #geom_smooth(model = lm, color = 'red')+
  labs(x = "Log Community size", y = "", title = "B")+
  scale_color_viridis(discrete = TRUE, name = "", option = "B", begin = 0.05, end = .75)+
  theme(plot.title = element_text(size = 20), axis.title = element_text(size = 18), axis.text = element_text(size = 16), legend.text = element_text(size = 14), legend.title = element_text(size = 14))+
  theme(legend.position = "none") 
pred_abuns #yellow = high variation in habhet, purple = low variation, low habhet 


p1 = gridExtra::grid.arrange(pred_plot, pred_abuns, ncol = 2, 
                  left = ggpubr::text_grob("Proportion Core Species in Community", 
                                  rot = 90, vjust = 1, size = 18))
ggsave(file = "output/Figure5.tiff", plot = p1)

####Figure 6####
#at top scales, with just variances - diamond shape figure that parallels prediction table (alt to outcome table)
scales_hetero = read.csv("intermed/core_scales_hetero.csv", header = TRUE) #pulling from core output

scales_hetero2 = scales_hetero %>% 
  filter(scale == 66) %>% 
  filter(dep == "elev.var" | dep == "ndvi.var") %>% 
  filter(ind == "PCA.curvature" | ind == "PCA.max" | ind == "PCA.mid"| ind == "PCA.min" | ind == "PCA.slope")

ggplot(scales_hetero2, aes(x = ind, y = corr_r))+
  geom_pointrange(aes(color = dep, shape = dep, ymin = lowr, ymax = uppr), size = 1.2, position = position_dodge(width = 0.35))+geom_abline(intercept = 0, slope = 0)+
  theme_classic()+theme(axis.title = element_text(size = 18), axis.text = element_text(size = 16), legend.position = c(0.55, 0.25), legend.text = element_text(size = 16), legend.title = element_text(size = 16))+
  labs(x = "Occupancy-scale parameters", y = "Pearson's correlation coefficient")+
  scale_x_discrete(limit = c("PCA.min", "PCA.mid","PCA.slope","PCA.curvature","PCA.max"),
                   labels = c(expression("p"["min"]), expression("Scale"[50]),"Slope","Curvature",expression("p"["max"])))+
  scale_y_continuous(breaks = c(-0.6, -0.4, -0.2, 0, 0.2, 0.4))+
  scale_color_manual(name = "Environmental Heterogeneity",
                     values=c("#440154FF", "#55C667FF"),
                     labels = c("Elevation", "NDVI"))+
  scale_shape_manual(name = "Environmental Heterogeneity",
                     values=c(16, 17),
                     labels = c("Elevation", "NDVI"))
#likely #440154FF purple and #55C667FF
ggsave(file = "output/Figure6.tiff", plot = last_plot())

####Figure 7####
#scales hetero derived at end of env_analysis script
scales_hetero_v = scales_hetero %>% 
  filter(ind == "PCA.curvature" | ind == "PCA.max" | ind == "PCA.mid"| ind == "PCA.min" | ind == "PCA.slope")

scales_hetero_v$ind = factor(scales_hetero_v$ind, 
                             levels = c("PCA.min","PCA.mid", "PCA.slope","PCA.curvature", "PCA.max"),
                             labels = c(as.character(expression("p"["min"])), as.character(expression("Scale"[50])), "Slope", "Curvature", as.character(expression("p"["max"]))))

scales_hetero_v$dep = factor(scales_hetero_v$dep, 
                             levels=c("elev.var", "ndvi.var"),
                             labels=c("Elevation", "NDVI"))

#scale on x and r on y, panel by coef of interest, line color by var measure
ggplot(scales_hetero_v, aes(x = scale, y = corr_r))+
  geom_line(aes(color = dep), size = 1.4)+facet_wrap(~ind, labeller = label_parsed)+
  theme_classic()+
  geom_abline(intercept = 0, slope = 0)+
  theme_classic()+theme(text = element_text(size = 18))+
  labs(color = "Environmental Heterogeneity", x = "Number of aggregated BBS Routes", y = "Pearson's correlation coefficient")+theme(legend.position = c(0.84, 0.20))+
  scale_color_viridis(begin = 0, end = 0.7, discrete = TRUE, option = "D")+
  scale_y_continuous(breaks = c(-0.6, -0.4, -0.2, 0, 0.2, 0.4))
ggsave(file = "output/Figure7.tiff", plot = last_plot())

########################################################################################################
####Supplemental figures####
####Supp figures: comparing raw and core_coef patterns between 3 different cutoff vals for core species####
# Alternate cutoffs of core and transient species in a community: are distributions changed?
#This scripts pulls in bbsallscales, bbsallscales_50, and bbs_allscales80, as well as 
#core_coefs, core_coefs_50, and core_coefs_80 for visual comparisons

####Merge 3 core_coefs versions, add new factor column variable delineating cutoff category####
core_coefs67 = read.csv("intermed/core_coefs.csv", header = TRUE) 
core_coefs50 = read.csv("intermed/core_coefs50.csv", header = TRUE) 
core_coefs80 = read.csv("intermed/core_coefs80.csv", header = TRUE) 

core_coefs50 = core_coefs50 %>% 
  mutate(cutoff_lvl = "Core 50%") #1

core_coefs67 = core_coefs67 %>% 
  mutate(cutoff_lvl = "Core 67%") #2

core_coefs80 = core_coefs80 %>% 
  mutate(cutoff_lvl = "Core 80%") #3

coefs_supp_pre = rbind(core_coefs50, core_coefs67) # could probably rbind or inner join, idk 
coefs_supp = rbind(coefs_supp_pre, core_coefs80)

coefs_supp$cutoff_lvl = factor(coefs_supp$cutoff_lvl, 
                               levels = c("Core 50%","Core 67%", "Core 80%"),
                               labels = c("50%","67%", "80%"))

#gather 

coefs_tidy = coefs_supp %>% 
  gather(key = parm,
         value = parm_val, 
         2:11)

write.csv(coefs_tidy, "intermed/coefs_tidy_supp.csv", row.names = FALSE)

####Supplemental figures: plotting differences in coef vals at diff cutoffs####
coefs_tidy = read.csv("intermed/coefs_tidy_supp.csv", header = TRUE)
coefs_tidyA = coefs_tidy %>% filter(parm == "PCA.min"| parm == "PCA.mid"| parm == "PCA.slope"| parm == "PCA.curvature"| parm == "PCA.max")
coefs_tidyN = coefs_tidy %>% filter(parm == "PCN.min"| parm == "PCN.mid"| parm == "PCN.slope"| parm == "PCN.curvature"| parm == "PCN.max")

coefs_tidyA$parm = factor(coefs_tidyA$parm,
                          levels = c("PCA.min","PCA.mid", "PCA.slope","PCA.curvature", "PCA.max"),
                          labels = c(as.character(expression("p"["min"])), as.character(expression("Scale"[50])), "Slope", "Curvature", as.character(expression("p"["max"]))))

coefs_tidyN$parm = factor(coefs_tidyN$parm,
                          levels = c("PCN.min","PCN.mid", "PCN.slope","PCN.curvature", "PCN.max"),
                          labels = c(as.character(expression("p"["min"])), as.character(expression("Scale"[50])), "Slope", "Curvature", as.character(expression("p"["max"]))))



ggplot(coefs_tidyA, aes(x = cutoff_lvl, y = parm_val))+
  geom_boxplot()+facet_wrap(~parm, scales = "free_y", labeller = label_parsed)+
  theme_classic()+theme(text = element_text(size = 18))+
  labs(x = "Proportion of Presence Required for Designation as Core", y = "Parameter values", title = "Parameters derived from proportion core-area scaling relationship")
ggsave(file = "output/Figure8.tiff", plot = last_plot())

ggplot(coefs_tidyN, aes(x = cutoff_lvl, y = parm_val))+
  geom_boxplot()+facet_wrap(~parm, scales = "free_y", labeller = label_parsed)+
  theme_classic()+theme(text = element_text(size = 18))+
  labs(x = "Proportion of Presence Required for Designation as Core", y = "Parameter values", title = "Parameters derived from proportion core-abundance scaling relationship")
ggsave(file = "output/Figure9.tiff", plot = last_plot())



####Summary stats for text####
bbs_allscales = read.csv("intermed/bbs_allscales.csv", header = TRUE)
bbs_allscales_minr = bbs_allscales %>% 
  dplyr::filter(scale == "seg5")
summary(bbs_allscales_minr$pctCore)
stats::quantile(bbs_allscales_minr$pctCore, probs = c(0.025, 0.975))


bbs_allscales_maxr = bbs_allscales %>% 
  dplyr::filter(scale == 66)
summary(bbs_allscales_maxr$pctCore)
stats::quantile(bbs_allscales_maxr$pctCore, probs = c(0.025, 0.975))


bbs_allscales_sing = bbs_allscales %>% 
  dplyr::filter(scale == "seg50")
summary(bbs_allscales_sing$pctCore)
stats::quantile(bbs_allscales_sing$pctCore, probs = c(0.025, 0.975))



####Plot stateroutes 1 by 1, pick out some emblematic "types"####
bbs_allscales = na.omit(read.csv("intermed/bbs_allscales.csv", header = TRUE))
focalrtes = unique(bbs_allscales$focalrte)
setwd("output/rte_imgs")

for (r in focalrtes) {
  bbs_allsub = bbs_allscales %>% filter(focalrte == r)
  ggplot(bbs_allsub, aes(x = logA, y = meanOcc))+
    geom_line()+ #coord_cartesian(xlim = c(0, 3.5), ylim = c(0, 1))+ tweak with 
    theme_classic()+ labs(x = "Log Area", y = "Mean Community Occupancy", 
                          title = r)
  ggsave(paste("plot", r, ".tiff", sep = ""))
}

dev.off()
